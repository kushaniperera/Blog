- Balancing User Privacy and Innovation in Augmented and Virtual Reality
  authors:: [[Elysse Dick]]
  type:: [[Legal]]
  category:: [[Literature]]  
  published-year:: 2021
  DOI:: na
	- [[card]] E Dick covers privacy issues in AR/VR in terms of four data types, Observed, Observable, Computed and Associated. Further identifies that AR/VR collects a substantial amount of data compared to other technologies due to the use of sensors which makes it difficult for a user to remain anonymous and retain their autonomy but this collected information is vital for the AR/VR applications to function.
	  card-last-interval:: 4
	  card-repeats:: 1
	  card-ease-factor:: 2.6
	  card-next-schedule:: 2022-10-24T01:41:15.804Z
	  card-last-reviewed:: 2022-10-20T01:41:15.805Z
	  card-last-score:: 5
- Policymakers should address privacy in AR/VR by considering the different types of information these devices collect and establishing appropriate safeguards to protect users against actual harms that may arise from this data collection.
	- Observable: information about an individual that AR/VR technologies as well as other third parties can both observe and replicate, such as digital media the individual produces or their digital communications;
	  ▪ Observed: information an individual provides or generates, which third parties can observe but not replicate, such as biographical information or location data;
	  ▪ Computed: new information AR/VR technologies infer by manipulating observable and observed data, such as biometric identification or advertising profiles; and
	  ▪ Associated: information that, on its own, does not provide descriptive details about an individual, such as a username or IP address.
- The unique challenges AR/VR technologies present, therefore, arise from the risks of aggregating sensitive information and the challenge of adapting mitigation measures that were designed for other consumer technologies into immersive, three-dimensional environments.
- policy responses that approach AR/VR as a monolith will almost certainly result in overregulation of certain types of data collection, while also leaving critical gaps in protections for others.
- Congress and relevant rulemaking bodies should create rules to safeguard against the potential for harm that arises from new forms of data collection, such as biometric identification and personal information inferred from biometric data, through transparency and choice requirements;
- Lawmakers should enact federal privacy legislation to harmonize compliance requirements at the national level rather than rely on state-by-state and sector-specific regulations; and
- Government agencies and industry should develop voluntary guidelines for AR/VR developers to secure users’ privacy through transparency and disclosure practices, user privacy controls (including opt-out mechanics), information security standards, and considerations for the unique risks presented by biometric identifying and biometrically derived data.
- Advanced functions, such as gaze-tracking and even brain-computer interface (BCI) technologies that interpret neural signals, continue to introduce new consumer data collection practices largely unique to AR/VR devices and applications.
- Unlike two-dimensional images, such as profile pictures or digital photographs, three-dimensional avatars such as those in fully immersive VR experiences are a digital embodiment of an individual, including their physical appearance, gestures, and mannerisms.4 Users experience these virtual bodies as they would their own in physical space—making this particular form of observable data more intimate than similar two-dimensional information.
- Most of the privacy concerns from observable data have to do with anonymity and personal autonomy—that is, individuals’ ability to control how much, or how little, others are able to identify and observe about them.
- simply concealing, anonymizing, or restricting collection of this data (observed data) would drastically reduce the quality of these services—or indeed render them effectively useless—and impede innovation of any technology that might require user-provided information.
- In many instances, particularly with highly sensitive information (e.g., biometric identifiers or sensitive health information), the AR/VR application may process the data entirely on the local device—and not transfer any data to a third party—or only store data in the cloud when the user fully controls the encryption keys, thereby reducing the risk of misuse. In others data can be shared or storedexternally without being observed by the human eye.
- disclosure and user consent form the foundation of any mitigation approach for computed data.
	- Privacy harms from computed data arise largely due to unintended use, unauthorized access, or malicious misuse.
- Because computed data could reveal information users may not wish to disclose, laws against discrimination based on such information are particularly important. However, nondiscrimination laws do not always address the risk of harms from incorrectly inferred information, such as adverse actions taken based on an inaccurate credit score. Allowing users to correct information that is untrue or out of date can further mitigate these risks.
- Disclosure, transparency, and consent standards can also mitigate risks of harm by notifying users when a product or service combines their associated data with other information, including identifying information.w
- This tracked “nonverbal data”—the subtle, subconscious movements sensors can detect—is virtually impossible for users to consciously control.35
	- As photorealistic avatars become more widely used, particularly for use cases outside of entertainment, virtual identities may more-closely mirror physical reality.34 This representation becomes even more accurate with the use of motion, gesture, and gaze-tracking, which replicate a user’s physical responses within their virtual environment. While the ability to mirror an individual’s expressions and reactions enhances interactions within virtual space, it also requires users to share—and allow devices to gather, track, and process—much more information than they would with other digital media platforms that simply transmit audiovisual information.
- once an application connects identifying information (e.g., a full name) with biometric identification data (e.g., from eye-tracking cameras), it is nearly impossible to fully anonymize a user even if the identifying information is removed.